{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0503ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from linear_regression import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3f9d283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, y, w, b):\n",
    "   \n",
    "    m = x.shape[0] \n",
    "    cost = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = cost + (f_wb - y[i])**2\n",
    "    total_cost = 1 / (2 * m) * cost\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "53413f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)    : model parameters  \n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
    "     \"\"\"\n",
    "    \n",
    "    # Number of training examples\n",
    "    m = x.shape[0]    \n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):  \n",
    "        f_wb = w * x[i] + b \n",
    "        dj_dw_i = (f_wb - y[i]) * x[i] \n",
    "        dj_db_i = f_wb - y[i] \n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i \n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f36edc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n",
    "    \"\"\"\n",
    "    Performs gradient descent to fit w,b. Updates w,b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,))  : Data, m examples \n",
    "      y (ndarray (m,))  : target values\n",
    "      w_in,b_in (scalar): initial values of model parameters  \n",
    "      alpha (float):     Learning rate\n",
    "      num_iters (int):   number of iterations to run gradient descent\n",
    "      cost_function:     function to call to produce cost\n",
    "      gradient_function: function to call to produce gradient\n",
    "      \n",
    "    Returns:\n",
    "      w (scalar): Updated value of parameter after running gradient descent\n",
    "      b (scalar): Updated value of parameter after running gradient descent\n",
    "      J_history (List): History of cost values\n",
    "      p_history (list): History of parameters [w,b] \n",
    "      \"\"\"\n",
    "    \n",
    "    w = copy.deepcopy(w_in) # avoid modifying global w_in\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w , b)     \n",
    "\n",
    "        # Update Parameters using equation (3) above\n",
    "        b = b - alpha * dj_db                            \n",
    "        w = w - alpha * dj_dw                            \n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(x, y, w , b))\n",
    "            p_history.append([w,b])\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    " \n",
    "    return w, b, J_history, p_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9bf3b1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[230.1,  22.1],\n",
       "       [ 44.5,  10.4],\n",
       "       [ 17.2,   9.3],\n",
       "       [151.5,  18.5],\n",
       "       [180.8,  12.9],\n",
       "       [  8.7,   7.2],\n",
       "       [ 57.5,  11.8],\n",
       "       [120.2,  13.2],\n",
       "       [  8.6,   4.8],\n",
       "       [199.8,  10.6],\n",
       "       [ 66.1,   8.6],\n",
       "       [214.7,  17.4],\n",
       "       [ 23.8,   9.2],\n",
       "       [ 97.5,   9.7],\n",
       "       [204.1,  19. ],\n",
       "       [195.4,  22.4],\n",
       "       [ 67.8,  12.5],\n",
       "       [281.4,  24.4],\n",
       "       [ 69.2,  11.3],\n",
       "       [147.3,  14.6],\n",
       "       [218.4,  18. ],\n",
       "       [237.4,  12.5],\n",
       "       [ 13.2,   5.6],\n",
       "       [228.3,  15.5],\n",
       "       [ 62.3,   9.7],\n",
       "       [262.9,  12. ],\n",
       "       [142.9,  15. ],\n",
       "       [240.1,  15.9],\n",
       "       [248.8,  18.9],\n",
       "       [ 70.6,  10.5],\n",
       "       [292.9,  21.4],\n",
       "       [112.9,  11.9],\n",
       "       [ 97.2,   9.6],\n",
       "       [265.6,  17.4],\n",
       "       [ 95.7,   9.5],\n",
       "       [290.7,  12.8],\n",
       "       [266.9,  25.4],\n",
       "       [ 74.7,  14.7],\n",
       "       [ 43.1,  10.1],\n",
       "       [228. ,  21.5],\n",
       "       [202.5,  16.6],\n",
       "       [177. ,  17.1],\n",
       "       [293.6,  20.7],\n",
       "       [206.9,  12.9],\n",
       "       [ 25.1,   8.5],\n",
       "       [175.1,  14.9],\n",
       "       [ 89.7,  10.6],\n",
       "       [239.9,  23.2],\n",
       "       [227.2,  14.8],\n",
       "       [ 66.9,   9.7],\n",
       "       [199.8,  11.4],\n",
       "       [100.4,  10.7],\n",
       "       [216.4,  22.6],\n",
       "       [182.6,  21.2],\n",
       "       [262.7,  20.2],\n",
       "       [198.9,  23.7],\n",
       "       [  7.3,   5.5],\n",
       "       [136.2,  13.2],\n",
       "       [210.8,  23.8],\n",
       "       [210.7,  18.4],\n",
       "       [ 53.5,   8.1],\n",
       "       [261.3,  24.2],\n",
       "       [239.3,  15.7],\n",
       "       [102.7,  14. ],\n",
       "       [131.1,  18. ],\n",
       "       [ 69. ,   9.3],\n",
       "       [ 31.5,   9.5],\n",
       "       [139.3,  13.4],\n",
       "       [237.4,  18.9],\n",
       "       [216.8,  22.3],\n",
       "       [199.1,  18.3],\n",
       "       [109.8,  12.4],\n",
       "       [ 26.8,   8.8],\n",
       "       [129.4,  11. ],\n",
       "       [213.4,  17. ],\n",
       "       [ 16.9,   8.7],\n",
       "       [ 27.5,   6.9],\n",
       "       [120.5,  14.2],\n",
       "       [  5.4,   5.3],\n",
       "       [116. ,  11. ],\n",
       "       [ 76.4,  11.8],\n",
       "       [239.8,  12.3],\n",
       "       [ 75.3,  11.3],\n",
       "       [ 68.4,  13.6],\n",
       "       [213.5,  21.7],\n",
       "       [193.2,  15.2],\n",
       "       [ 76.3,  12. ],\n",
       "       [110.7,  16. ],\n",
       "       [ 88.3,  12.9],\n",
       "       [109.8,  16.7],\n",
       "       [134.3,  11.2],\n",
       "       [ 28.6,   7.3],\n",
       "       [217.7,  19.4],\n",
       "       [250.9,  22.2],\n",
       "       [107.4,  11.5],\n",
       "       [163.3,  16.9],\n",
       "       [197.6,  11.7],\n",
       "       [184.9,  15.5],\n",
       "       [289.7,  25.4],\n",
       "       [135.2,  17.2],\n",
       "       [222.4,  11.7],\n",
       "       [296.4,  23.8],\n",
       "       [280.2,  14.8],\n",
       "       [187.9,  14.7],\n",
       "       [238.2,  20.7],\n",
       "       [137.9,  19.2],\n",
       "       [ 25. ,   7.2],\n",
       "       [ 90.4,   8.7],\n",
       "       [ 13.1,   5.3],\n",
       "       [255.4,  19.8],\n",
       "       [225.8,  13.4],\n",
       "       [241.7,  21.8],\n",
       "       [175.7,  14.1],\n",
       "       [209.6,  15.9],\n",
       "       [ 78.2,  14.6],\n",
       "       [ 75.1,  12.6],\n",
       "       [139.2,  12.2],\n",
       "       [ 76.4,   9.4],\n",
       "       [125.7,  15.9],\n",
       "       [ 19.4,   6.6],\n",
       "       [141.3,  15.5],\n",
       "       [ 18.8,   7. ],\n",
       "       [224. ,  11.6],\n",
       "       [123.1,  15.2],\n",
       "       [229.5,  19.7],\n",
       "       [ 87.2,  10.6],\n",
       "       [  7.8,   6.6],\n",
       "       [ 80.2,   8.8],\n",
       "       [220.3,  24.7],\n",
       "       [ 59.6,   9.7],\n",
       "       [  0.7,   1.6],\n",
       "       [265.2,  12.7],\n",
       "       [  8.4,   5.7],\n",
       "       [219.8,  19.6],\n",
       "       [ 36.9,  10.8],\n",
       "       [ 48.3,  11.6],\n",
       "       [ 25.6,   9.5],\n",
       "       [273.7,  20.8],\n",
       "       [ 43. ,   9.6],\n",
       "       [184.9,  20.7],\n",
       "       [ 73.4,  10.9],\n",
       "       [193.7,  19.2],\n",
       "       [220.5,  20.1],\n",
       "       [104.6,  10.4],\n",
       "       [ 96.2,  11.4],\n",
       "       [140.3,  10.3],\n",
       "       [240.1,  13.2],\n",
       "       [243.2,  25.4],\n",
       "       [ 38. ,  10.9],\n",
       "       [ 44.7,  10.1],\n",
       "       [280.7,  16.1],\n",
       "       [121. ,  11.6],\n",
       "       [197.6,  16.6],\n",
       "       [171.3,  19. ],\n",
       "       [187.8,  15.6],\n",
       "       [  4.1,   3.2],\n",
       "       [ 93.9,  15.3],\n",
       "       [149.8,  10.1],\n",
       "       [ 11.7,   7.3],\n",
       "       [131.7,  12.9],\n",
       "       [172.5,  14.4],\n",
       "       [ 85.7,  13.3],\n",
       "       [188.4,  14.9],\n",
       "       [163.5,  18. ],\n",
       "       [117.2,  11.9],\n",
       "       [234.5,  11.9],\n",
       "       [ 17.9,   8. ],\n",
       "       [206.8,  12.2],\n",
       "       [215.4,  17.1],\n",
       "       [284.3,  15. ],\n",
       "       [ 50. ,   8.4],\n",
       "       [164.5,  14.5],\n",
       "       [ 19.6,   7.6],\n",
       "       [168.4,  11.7],\n",
       "       [222.4,  11.5],\n",
       "       [276.9,  27. ],\n",
       "       [248.4,  20.2],\n",
       "       [170.2,  11.7],\n",
       "       [276.7,  11.8],\n",
       "       [165.6,  12.6],\n",
       "       [156.6,  10.5],\n",
       "       [218.5,  12.2],\n",
       "       [ 56.2,   8.7],\n",
       "       [287.6,  26.2],\n",
       "       [253.8,  17.6],\n",
       "       [205. ,  22.6],\n",
       "       [139.5,  10.3],\n",
       "       [191.1,  17.3],\n",
       "       [286. ,  15.9],\n",
       "       [ 18.7,   6.7],\n",
       "       [ 39.5,  10.8],\n",
       "       [ 75.5,   9.9],\n",
       "       [ 17.2,   5.9],\n",
       "       [166.8,  19.6],\n",
       "       [149.7,  17.3],\n",
       "       [ 38.2,   7.6],\n",
       "       [ 94.2,   9.7],\n",
       "       [177. ,  12.8],\n",
       "       [283.6,  25.5],\n",
       "       [232.1,  13.4]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"./data from book/advertising.csv\"\n",
    "data = pd.read_csv(file)\n",
    "data = data.drop(columns=[\"Unnamed: 0\",\"radio\",\"newspaper\"])\n",
    "\n",
    "data = np.array(data)\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5ac4d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding gradiant of j \n",
    "#assume w and b for statrting w=0 b=0\n",
    "# def totals(data):\n",
    "#     x_total = 0\n",
    "#     y_total = 0\n",
    "#     for d in data:\n",
    "#         x_total += d[0]\n",
    "#         y_total += d[1]\n",
    "#     return x_total,y_total\n",
    "\n",
    "#  gradient of j = submission of [1/m (w*d[0] + b -d[1])(d[0] + 1)]\n",
    "\n",
    "def gradiant(data,w,b):\n",
    "    j_i=0\n",
    "    j_j = 0\n",
    "    m = data.shape[0]\n",
    "    \n",
    "    # calulating i of gradiant of j\n",
    "#     co = 0\n",
    "#     for d in data:\n",
    "#         co+=1\n",
    "#         k = (w*d[0] + b -d[1])\n",
    "#         if abs(k)>4.067235027595472e+303:\n",
    "#             break\n",
    "#         j_i += k*(d[0])\n",
    "#         j_j += k\n",
    "\n",
    "#         print(\"x: \",d[0],\"y ;\",d[1,\"w :\",w,\"b: \", b,\"k:  \",k)\n",
    "\n",
    "        \n",
    "    # print(\" j_i :\",j_i,\"j_j :\",j_j)\n",
    "    grad = [1/m * j_i,1/m *j_j]\n",
    "    return grad\n",
    "\n",
    "def travel(grad,dist,w,b):\n",
    "    w_n=0 \n",
    "    b_n = 0\n",
    "    \n",
    "    w_n = w-dist*grad[0]\n",
    "    b_n = b-dist*grad[1]\n",
    "    return w_n,b_n\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # ′(μ)=−2∑ni=1(xi−μ) and f′′(μ)=2n\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "67d2fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n",
    "    \"\"\"\n",
    "    Performs gradient descent to fit w,b. Updates w,b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,))  : Data, m examples \n",
    "      y (ndarray (m,))  : target values\n",
    "      w_in,b_in (scalar): initial values of model parameters  \n",
    "      alpha (float):     Learning rate\n",
    "      num_iters (int):   number of iterations to run gradient descent\n",
    "      cost_function:     function to call to produce cost\n",
    "      gradient_function: function to call to produce gradient\n",
    "      \n",
    "    Returns:\n",
    "      w (scalar): Updated value of parameter after running gradient descent\n",
    "      b (scalar): Updated value of parameter after running gradient descent\n",
    "      J_history (List): History of cost values\n",
    "      p_history (list): History of parameters [w,b] \n",
    "      \"\"\"\n",
    "    \n",
    "    w = (w_in) # avoid modifying global w_in\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w , b)     \n",
    "\n",
    "        # Update Parameters using equation (3) above\n",
    "        b = b - alpha * dj_db                            \n",
    "        w = w - alpha * dj_dw                            \n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(x, y, w , b))\n",
    "            p_history.append([w,b])\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    " \n",
    "    return w, b, J_history, p_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b902939f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 8.36e+06  dj_dw: -2.411e+03, dj_db: -1.402e+01   w:  2.411e+01, b: 1.40225e-01\n",
      "Iteration   50: Cost 8.83e+252  dj_dw: -2.478e+126, dj_db: -1.259e+124   w:  2.470e+124, b: 1.25427e+122\n",
      "Iteration  100: Cost inf  dj_dw: -2.548e+249, dj_db: -1.294e+247   w:  2.539e+247, b: 1.28959e+245\n",
      "Iteration  150: Cost nan  dj_dw:  nan, dj_db:  nan   w:  nan, b: nan\n",
      "Iteration  200: Cost nan  dj_dw:  nan, dj_db:  nan   w:  nan, b: nan\n",
      "Iteration  250: Cost nan  dj_dw:  nan, dj_db:  nan   w:  nan, b: nan\n",
      "Iteration  300: Cost nan  dj_dw:  nan, dj_db:  nan   w:  nan, b: nan\n",
      "Iteration  350: Cost nan  dj_dw:  nan, dj_db:  nan   w:  nan, b: nan\n",
      "Iteration  400: Cost nan  dj_dw:  nan, dj_db:  nan   w:  nan, b: nan\n",
      "Iteration  450: Cost nan  dj_dw:  nan, dj_db:  nan   w:  nan, b: nan\n",
      "Iteration    0: Cost 8.36e+06  dj_dw: -2.411e+03, dj_db: -1.402e+01   w:  2.411e+01, b: 1.40225e-01\n",
      "Iteration   50: Cost 8.83e+252  dj_dw: -2.478e+126, dj_db: -1.259e+124   w:  2.470e+124, b: 1.25427e+122\n",
      "Iteration  100: Cost inf  dj_dw: -2.548e+249, dj_db: -1.294e+247   w:  2.539e+247, b: 1.28959e+245\n",
      "Iteration  150: Cost nan  dj_dw:  nan, dj_db:  nan   w:  nan, b: nan\n",
      "Iteration  200: Cost nan  dj_dw:  nan, dj_db:  nan   w:  nan, b: nan\n",
      "Iteration  250: Cost nan  dj_dw:  nan, dj_db:  nan   w:  nan, b: nan\n",
      "Iteration  300: Cost nan  dj_dw:  nan, dj_db:  nan   w:  nan, b: nan\n",
      "Iteration  350: Cost nan  dj_dw:  nan, dj_db:  nan   w:  nan, b: nan\n",
      "Iteration  400: Cost nan  dj_dw:  nan, dj_db:  nan   w:  nan, b: nan\n",
      "Iteration  450: Cost nan  dj_dw:  nan, dj_db:  nan   w:  nan, b: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DINO\\AppData\\Local\\Temp\\ipykernel_16656\\2885804429.py:8: RuntimeWarning: overflow encountered in scalar add\n",
      "  cost = cost + (f_wb - y[i])**2\n",
      "C:\\Users\\DINO\\AppData\\Local\\Temp\\ipykernel_16656\\2885804429.py:8: RuntimeWarning: overflow encountered in scalar power\n",
      "  cost = cost + (f_wb - y[i])**2\n",
      "C:\\Users\\DINO\\AppData\\Local\\Temp\\ipykernel_16656\\255131020.py:23: RuntimeWarning: overflow encountered in scalar add\n",
      "  dj_dw += dj_dw_i\n",
      "C:\\Users\\DINO\\AppData\\Local\\Temp\\ipykernel_16656\\1598022462.py:35: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  w = w - alpha * dj_dw\n"
     ]
    }
   ],
   "source": [
    "# gradient_descent(data[:,0],data[:,1],0,0,1.0e-2,500,compute_cost,compute_gradient)\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(data[:,0],data[:,1],0,0,1.0e-2,500,compute_cost,compute_gradient)\n",
    "w_final,b_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd17a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let inital w=0 and b=0\n",
    "w=0\n",
    "b=0\n",
    "dist = 0.6\n",
    "for i in range(50):\n",
    "    # print(gradiant(data,w,b))\n",
    "#     w,b  =travel(coumant(data,w,b),dist,w,b)\n",
    "    dj_dw,dj_db = compute_gradient(data[:,0],data[:,1],w,b)\n",
    "    w = w-(dist*dj_dw)\n",
    "    b = b-(dist*dj_db)\n",
    "    \n",
    "    \n",
    "print(w,b)\n",
    "jb = predict_graph(data,w,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2205527",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_x, min_y, max_x, max_y = range_w_b(data)\n",
    "min_j,per_w,per_b = func_min_j(data , min_x, min_y, max_x, max_y)\n",
    "jh = predict_graph(data,per_w,per_b)\n",
    "print(per_w,per_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29177c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "jb = predict_graph(data,4.602253368777066e+157,2.3371401340462734e+155)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db62f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pl\n",
    "# data\n",
    "pl.scatter(data[:,0],data[:,1])\n",
    "pl.plot(jh[:,0],jh[:,1],color='black')\n",
    "pl.plot(jb[:,0],jb[:,1],color='red')\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6ce7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0b961e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
